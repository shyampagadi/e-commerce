# =============================================================================
# PETCLINIC CHAOS ENGINEERING EXPERIMENT - COMPREHENSIVE RESILIENCE TESTING DOCUMENTATION
# =============================================================================
# This file defines a chaos engineering experiment using Chaos Mesh to test the
# resilience and fault tolerance of the PetClinic microservices architecture.
# Chaos engineering proactively identifies weaknesses by introducing controlled
# failures in a production-like environment.
#
# RESILIENCE PHILOSOPHY: "Everything fails all the time" - this experiment validates
# that the system can gracefully handle service failures and maintain availability.
#
# PRODUCTION IMPACT: This experiment directly tests the system's ability to handle
# real-world failures, improving overall system reliability and user experience.
# =============================================================================

# -----------------------------------------------------------------------------
# CHAOS MESH POD CHAOS EXPERIMENT - SERVICE FAILURE SIMULATION
# -----------------------------------------------------------------------------
# API version for Chaos Mesh PodChaos resources
# chaos-mesh.org/v1alpha1 is the current API version for Chaos Mesh experiments
# Chaos Mesh extends Kubernetes with Custom Resource Definitions for chaos experiments
apiVersion: chaos-mesh.org/v1alpha1

# Resource type: PodChaos simulates pod-level failures and disruptions
# PodChaos can kill pods, make them fail, or introduce various pod-level faults
# This is one of the most common chaos engineering experiments for testing resilience
kind: PodChaos

# Metadata section for the chaos experiment
metadata:
  # Name of the chaos experiment - should be descriptive of the failure scenario
  # Naming convention: <target-service>-<failure-type>
  name: customers-service-failure
  # EXPERIMENT IDENTIFICATION: Clear naming helps with experiment tracking and analysis
  
  # Namespace where the chaos experiment runs
  # Should match the namespace of the target application
  namespace: petclinic
  # SCOPE CONTROL: Ensures experiment only affects the intended application namespace

# Specification section defining the chaos experiment parameters
spec:
  # Action defines the type of chaos to introduce
  # pod-kill terminates selected pods to simulate unexpected failures
  action: pod-kill
  # FAILURE TYPE: Simulates the most common production failure - pod crashes
  # REAL-WORLD SCENARIO: Mimics node failures, OOM kills, or application crashes
  
  # Mode determines how many targets are affected by the experiment
  # "one" means only one pod matching the selector will be affected
  mode: one
  # BLAST RADIUS CONTROL: Limits impact to single pod to prevent total service outage
  # SAFETY: Ensures experiment doesn't completely disable the service
  # ALTERNATIVE MODES: "all" (affects all matching pods), "fixed" (specific number), "percentage" (percentage of pods)
  
  # Duration specifies how long the chaos effect lasts
  # For pod-kill, this is the interval before the pod is allowed to restart
  duration: "30s"
  # RECOVERY TIME: 30 seconds allows observation of failure impact and recovery
  # KUBERNETES RECOVERY: Deployment controller will recreate killed pods automatically
  # MONITORING WINDOW: Provides sufficient time to observe metrics and alerts
  
  # Selector defines which pods are targeted by the chaos experiment
  # Uses label selectors to identify target pods precisely
  selector:
    # Namespaces array specifies which namespaces to search for target pods
    # Multiple namespaces can be specified for cross-namespace experiments
    namespaces:
      - petclinic
    # NAMESPACE ISOLATION: Ensures experiment only affects intended application
    
    # Label selectors identify specific pods within the target namespaces
    # Uses key-value pairs to match pod labels
    labelSelectors:
      # Target pods with the "app" label set to "customers-service"
      "app": "customers-service"
      # SERVICE TARGETING: Specifically tests the customers microservice
      # REQUIREMENT: Customer service pods must have this label for targeting
      # MICROSERVICE FOCUS: Tests resilience of a critical business service
  
  # Scheduler defines when and how often the chaos experiment runs
  # Supports cron-like scheduling for automated, recurring experiments
  scheduler:
    # Cron expression for experiment scheduling
    # "@every 5m" runs the experiment every 5 minutes
    cron: "@every 5m"
    # FREQUENCY: Regular testing ensures continuous resilience validation
    # AUTOMATION: Removes human intervention from resilience testing
    # PRODUCTION CONSIDERATION: 5-minute intervals may be too frequent for production

# =============================================================================
# CHAOS ENGINEERING ANALYSIS AND RESILIENCE VALIDATION
# =============================================================================
#
# EXPERIMENT OBJECTIVES:
# 1. VALIDATE SERVICE RESILIENCE: Test if customers-service can handle pod failures
# 2. TEST RECOVERY MECHANISMS: Verify Kubernetes automatically restarts failed pods
# 3. ASSESS IMPACT RADIUS: Measure how pod failure affects overall system functionality
# 4. VALIDATE MONITORING: Ensure alerts and monitoring detect and report failures
# 5. TEST CLIENT RESILIENCE: Verify other services handle customers-service unavailability
#
# EXPECTED SYSTEM BEHAVIORS:
# ✅ KUBERNETES RECOVERY: Deployment should automatically recreate killed pods
# ✅ SERVICE CONTINUITY: Other customers-service pods should continue serving requests
# ✅ LOAD BALANCING: Service should route traffic away from failed pods
# ✅ CIRCUIT BREAKERS: Dependent services should activate circuit breakers
# ✅ GRACEFUL DEGRADATION: System should maintain core functionality
# ✅ MONITORING ALERTS: Monitoring systems should detect and alert on failures
#
# POTENTIAL FAILURE SCENARIOS TO OBSERVE:
# ❌ CASCADING FAILURES: Failure spreads to other services
# ❌ DATA INCONSISTENCY: Database transactions left in inconsistent state
# ❌ SESSION LOSS: User sessions lost without proper session management
# ❌ SLOW RECOVERY: Pods take too long to restart and become ready
# ❌ MONITORING GAPS: Failures not detected or alerted properly
#
# PRODUCTION RECOMMENDATIONS:
#
# 1. EXPERIMENT SCHEDULING OPTIMIZATION:
#    # Less frequent scheduling for production environments
#    scheduler:
#      cron: "0 2 * * 1"  # Weekly on Monday at 2 AM
#    
#    # Or manual execution for controlled testing
#    # Remove scheduler section for manual-only execution
#
# 2. ENHANCED EXPERIMENT CONFIGURATION:
#    # Multiple failure modes for comprehensive testing
#    apiVersion: chaos-mesh.org/v1alpha1
#    kind: PodChaos
#    metadata:
#      name: customers-service-comprehensive-failure
#      namespace: petclinic
#    spec:
#      action: pod-failure  # Make pods fail without killing
#      mode: percentage
#      value: "25"          # Affect 25% of pods
#      duration: "2m"       # Longer duration for extended testing
#      selector:
#        namespaces:
#          - petclinic
#        labelSelectors:
#          "app": "customers-service"
#
# 3. NETWORK CHAOS EXPERIMENTS:
#    # Test network partitions and latency
#    apiVersion: chaos-mesh.org/v1alpha1
#    kind: NetworkChaos
#    metadata:
#      name: customers-service-network-delay
#      namespace: petclinic
#    spec:
#      action: delay
#      mode: one
#      selector:
#        namespaces:
#          - petclinic
#        labelSelectors:
#          "app": "customers-service"
#      delay:
#        latency: "100ms"
#        correlation: "100"
#        jitter: "0ms"
#      duration: "5m"
#
# 4. STRESS TESTING EXPERIMENTS:
#    # Test resource exhaustion scenarios
#    apiVersion: chaos-mesh.org/v1alpha1
#    kind: StressChaos
#    metadata:
#      name: customers-service-memory-stress
#      namespace: petclinic
#    spec:
#      mode: one
#      selector:
#        namespaces:
#          - petclinic
#        labelSelectors:
#          "app": "customers-service"
#      duration: "3m"
#      stressors:
#        memory:
#          workers: 1
#          size: "512MB"
#
# MONITORING AND OBSERVABILITY REQUIREMENTS:
#
# 1. EXPERIMENT MONITORING:
#    - Monitor experiment execution status and results
#    - Track experiment impact on system metrics
#    - Measure recovery time and system resilience
#    - Alert on unexpected experiment behaviors
#
# 2. SYSTEM HEALTH MONITORING:
#    - Monitor service availability during experiments
#    - Track error rates and response times
#    - Monitor database connection health
#    - Observe circuit breaker activations
#
# 3. BUSINESS IMPACT MONITORING:
#    - Monitor user experience metrics during experiments
#    - Track business transaction success rates
#    - Measure customer impact and satisfaction
#    - Monitor revenue impact of failures
#
# SAFETY MEASURES AND GUARDRAILS:
#
# 1. EXPERIMENT SAFETY:
#    - Start with single pod targeting (mode: one)
#    - Use short durations for initial experiments
#    - Implement experiment abort mechanisms
#    - Schedule experiments during low-traffic periods
#
# 2. PRODUCTION SAFEGUARDS:
#    - Implement experiment approval workflows
#    - Use feature flags to disable experiments quickly
#    - Monitor business metrics during experiments
#    - Have rollback procedures ready
#
# 3. TEAM READINESS:
#    - Ensure on-call teams are aware of scheduled experiments
#    - Provide experiment runbooks and procedures
#    - Train teams on experiment analysis and response
#    - Establish communication channels for experiment coordination
#
# EXPERIMENT ANALYSIS AND LEARNING:
#
# 1. SUCCESS CRITERIA:
#    - System maintains availability during pod failures
#    - Recovery time meets defined SLA requirements
#    - No cascading failures or data corruption
#    - Monitoring and alerting function correctly
#
# 2. FAILURE ANALYSIS:
#    - Document any unexpected behaviors or failures
#    - Identify system weaknesses and improvement opportunities
#    - Update system design based on experiment learnings
#    - Enhance monitoring and alerting based on gaps identified
#
# 3. CONTINUOUS IMPROVEMENT:
#    - Gradually increase experiment complexity and scope
#    - Add new failure scenarios based on production incidents
#    - Automate experiment analysis and reporting
#    - Integrate chaos engineering into development lifecycle
#
# COMPLIANCE AND GOVERNANCE:
# - Chaos experiments should align with change management processes
# - Document experiment results for audit and compliance
# - Ensure experiments don't violate regulatory requirements
# - Maintain experiment logs for incident investigation
#
# =============================================================================
