# üöÄ Future of Containers

## üìã Learning Objectives
By the end of this module, you will:
- **Understand** emerging container technologies and future trends
- **Explore** WebAssembly, serverless containers, and edge computing
- **Analyze** the evolution of container orchestration and AI integration
- **Prepare** for next-generation containerization strategies
- **Implement** cutting-edge container technologies for e-commerce

## üéØ Real-World Context
The container ecosystem is rapidly evolving with new technologies that promise better performance, security, and developer experience. Understanding these trends helps e-commerce platforms stay competitive and prepare for future technological shifts.

---

## üìö Part 1: Container Technology Evolution

### Current State vs. Future Vision

**Current Container Limitations:**
- Resource overhead from full OS virtualization
- Security concerns with shared kernel
- Cold start times in serverless environments
- Complex orchestration requirements

**Future Container Innovations:**
- WebAssembly for near-native performance
- Serverless containers with instant scaling
- Edge computing integration
- AI-driven orchestration and optimization

### Technology Timeline

**2024-2025: Near-term Innovations**
- WebAssembly System Interface (WASI) maturation
- Improved serverless container platforms
- Enhanced security with confidential computing
- Better developer experience tools

**2026-2027: Medium-term Evolution**
- AI-powered container optimization
- Quantum-safe container security
- Advanced edge computing integration
- Unified multi-cloud container platforms

**2028+: Long-term Vision**
- Self-healing container ecosystems
- Predictive scaling and resource management
- Seamless hybrid cloud-edge deployments
- Autonomous container lifecycle management

---

## üåê Part 2: WebAssembly and Containers

### WebAssembly Container Integration

**Why WebAssembly Matters:**
- Near-native performance
- Language agnostic runtime
- Enhanced security sandbox
- Smaller binary sizes
- Faster cold starts

**WebAssembly E-Commerce Implementation:**
```dockerfile
# Dockerfile.wasm - WebAssembly-based e-commerce service
FROM scratch

# Copy WebAssembly binary
COPY target/wasm32-wasi/release/ecommerce-service.wasm /app.wasm

# Use WebAssembly runtime
ENTRYPOINT ["wasmtime", "/app.wasm"]
```

**WASM Service Example:**
```rust
// ecommerce-wasm-service/src/main.rs
use wasi_common::sync::WasiCtxBuilder;
use wasmtime::*;
use serde::{Deserialize, Serialize};

#[derive(Serialize, Deserialize)]
struct Product {
    id: u32,
    name: String,
    price: f64,
    category: String,
}

#[derive(Serialize, Deserialize)]
struct Order {
    id: u32,
    user_id: u32,
    products: Vec<Product>,
    total: f64,
}

// WebAssembly exported functions
#[no_mangle]
pub extern "C" fn process_order(order_data: *const u8, len: usize) -> i32 {
    // Process order logic
    let order_bytes = unsafe { std::slice::from_raw_parts(order_data, len) };
    
    match serde_json::from_slice::<Order>(order_bytes) {
        Ok(order) => {
            // Validate order
            if validate_order(&order) {
                // Process payment
                if process_payment(&order) {
                    // Update inventory
                    update_inventory(&order);
                    return 1; // Success
                }
            }
            0 // Failure
        }
        Err(_) => -1 // Invalid input
    }
}

fn validate_order(order: &Order) -> bool {
    !order.products.is_empty() && order.total > 0.0
}

fn process_payment(order: &Order) -> bool {
    // Simplified payment processing
    order.total <= 10000.0 // Max order limit
}

fn update_inventory(order: &Order) {
    // Update inventory logic
    for product in &order.products {
        println!("Updating inventory for product: {}", product.name);
    }
}

fn main() {
    println!("E-Commerce WebAssembly Service Started");
}
```

**WASM Container Orchestration:**
```yaml
# wasm-deployment.yml - WebAssembly container deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ecommerce-wasm-service
spec:
  replicas: 10
  selector:
    matchLabels:
      app: ecommerce-wasm
  template:
    metadata:
      labels:
        app: ecommerce-wasm
    spec:
      runtimeClassName: wasmtime
      containers:
      - name: ecommerce-service
        image: ecommerce/wasm-service:latest
        resources:
          requests:
            cpu: 10m      # Much lower resource requirements
            memory: 16Mi
          limits:
            cpu: 100m
            memory: 64Mi
        env:
        - name: WASM_RUNTIME
          value: "wasmtime"
```

---

## ‚òÅÔ∏è Part 3: Serverless Containers

### Next-Generation Serverless Platforms

**Serverless Container Evolution:**
- Instant cold starts (sub-100ms)
- Pay-per-request pricing
- Automatic scaling to zero
- Event-driven architectures

**Advanced Serverless E-Commerce:**
```yaml
# serverless-ecommerce.yml - Next-gen serverless containers
apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: ecommerce-product-api
spec:
  template:
    metadata:
      annotations:
        # Scale to zero when idle
        autoscaling.knative.dev/minScale: "0"
        autoscaling.knative.dev/maxScale: "1000"
        # Fast cold start optimization
        run.googleapis.com/cpu-throttling: "false"
        run.googleapis.com/execution-environment: gen2
    spec:
      containerConcurrency: 100
      containers:
      - image: ecommerce/product-api:serverless
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 1000m
            memory: 512Mi
        env:
        - name: SERVERLESS_MODE
          value: "true"
        - name: COLD_START_OPTIMIZATION
          value: "enabled"

---
apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: ecommerce-order-processor
spec:
  template:
    metadata:
      annotations:
        autoscaling.knative.dev/minScale: "1"  # Keep warm for critical service
        autoscaling.knative.dev/maxScale: "500"
    spec:
      containers:
      - image: ecommerce/order-processor:serverless
        resources:
          requests:
            cpu: 200m
            memory: 256Mi
```

**Serverless Container Optimization:**
```javascript
// serverless-optimization.js - Optimized for serverless
const express = require('express');

// Global connection pooling for serverless
let dbPool;
let redisClient;

const initializeConnections = async () => {
    if (!dbPool) {
        const { Pool } = require('pg');
        dbPool = new Pool({
            connectionString: process.env.DATABASE_URL,
            max: 1, // Minimal connections for serverless
            idleTimeoutMillis: 30000,
            connectionTimeoutMillis: 2000,
        });
    }
    
    if (!redisClient) {
        const redis = require('redis');
        redisClient = redis.createClient({
            url: process.env.REDIS_URL,
            socket: {
                connectTimeout: 1000,
                lazyConnect: true
            }
        });
        await redisClient.connect();
    }
};

const app = express();

// Middleware for serverless optimization
app.use((req, res, next) => {
    // Add serverless headers
    res.set('X-Serverless-Runtime', 'container');
    res.set('X-Cold-Start', process.env.COLD_START || 'false');
    next();
});

// Health check optimized for serverless
app.get('/health', (req, res) => {
    res.json({ 
        status: 'healthy', 
        runtime: 'serverless-container',
        timestamp: new Date().toISOString()
    });
});

// Product API with connection reuse
app.get('/api/products', async (req, res) => {
    try {
        await initializeConnections();
        
        // Check cache first
        const cacheKey = `products:${JSON.stringify(req.query)}`;
        const cached = await redisClient.get(cacheKey);
        
        if (cached) {
            return res.json(JSON.parse(cached));
        }
        
        // Query database
        const result = await dbPool.query('SELECT * FROM products LIMIT 20');
        const products = result.rows;
        
        // Cache for 5 minutes
        await redisClient.setex(cacheKey, 300, JSON.stringify(products));
        
        res.json(products);
    } catch (error) {
        console.error('Error:', error);
        res.status(500).json({ error: 'Internal server error' });
    }
});

// Graceful shutdown for serverless
process.on('SIGTERM', async () => {
    console.log('Received SIGTERM, shutting down gracefully');
    
    if (dbPool) {
        await dbPool.end();
    }
    
    if (redisClient) {
        await redisClient.quit();
    }
    
    process.exit(0);
});

const port = process.env.PORT || 8080;
app.listen(port, () => {
    console.log(`Serverless container listening on port ${port}`);
});
```

---

## üåç Part 4: Edge Computing Integration

### Container-Based Edge Computing

**Edge Computing Benefits:**
- Reduced latency for users
- Improved data privacy
- Bandwidth optimization
- Offline capability

**Edge E-Commerce Architecture:**
```yaml
# edge-deployment.yml - Edge computing for e-commerce
apiVersion: v1
kind: ConfigMap
metadata:
  name: edge-config
data:
  edge-locations: |
    - name: us-west-edge
      location: "San Francisco, CA"
      services: ["product-catalog", "user-session"]
    - name: us-east-edge
      location: "New York, NY"
      services: ["product-catalog", "order-processing"]
    - name: eu-edge
      location: "London, UK"
      services: ["product-catalog", "payment-processing"]

---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: edge-product-catalog
spec:
  selector:
    matchLabels:
      app: edge-product-catalog
  template:
    metadata:
      labels:
        app: edge-product-catalog
    spec:
      containers:
      - name: product-catalog
        image: ecommerce/product-catalog:edge
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
        env:
        - name: EDGE_MODE
          value: "true"
        - name: SYNC_INTERVAL
          value: "300" # Sync with central every 5 minutes
        - name: CACHE_SIZE
          value: "256MB"
        volumeMounts:
        - name: edge-cache
          mountPath: /cache
      volumes:
      - name: edge-cache
        emptyDir:
          sizeLimit: 1Gi
```

**Edge Synchronization Service:**
```javascript
// edge-sync.js - Edge data synchronization
class EdgeSyncService {
    constructor(centralUrl, edgeLocation) {
        this.centralUrl = centralUrl;
        this.edgeLocation = edgeLocation;
        this.cache = new Map();
        this.syncInterval = 5 * 60 * 1000; // 5 minutes
        this.lastSync = null;
    }
    
    async startSync() {
        console.log(`üåç Starting edge sync for ${this.edgeLocation}`);
        
        // Initial sync
        await this.syncFromCentral();
        
        // Periodic sync
        setInterval(async () => {
            await this.syncFromCentral();
        }, this.syncInterval);
        
        // Sync changes back to central
        setInterval(async () => {
            await this.syncToCenter();
        }, this.syncInterval / 2);
    }
    
    async syncFromCentral() {
        try {
            console.log('üì• Syncing data from central...');
            
            const response = await fetch(`${this.centralUrl}/api/sync/products`, {
                headers: {
                    'X-Edge-Location': this.edgeLocation,
                    'X-Last-Sync': this.lastSync || '0'
                }
            });
            
            const data = await response.json();
            
            // Update local cache
            for (const product of data.products) {
                this.cache.set(`product:${product.id}`, product);
            }
            
            this.lastSync = new Date().toISOString();
            console.log(`‚úÖ Synced ${data.products.length} products`);
            
        } catch (error) {
            console.error('‚ùå Sync from central failed:', error);
        }
    }
    
    async syncToCenter() {
        try {
            // Collect local changes (views, interactions, etc.)
            const localChanges = this.collectLocalChanges();
            
            if (localChanges.length === 0) return;
            
            console.log('üì§ Syncing local changes to central...');
            
            await fetch(`${this.centralUrl}/api/sync/edge-data`, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'X-Edge-Location': this.edgeLocation
                },
                body: JSON.stringify({
                    location: this.edgeLocation,
                    changes: localChanges,
                    timestamp: new Date().toISOString()
                })
            });
            
            console.log(`‚úÖ Synced ${localChanges.length} local changes`);
            
        } catch (error) {
            console.error('‚ùå Sync to central failed:', error);
        }
    }
    
    collectLocalChanges() {
        // Collect analytics, user interactions, inventory changes
        return [
            {
                type: 'product_view',
                productId: '123',
                count: 45,
                timestamp: new Date().toISOString()
            },
            {
                type: 'search_query',
                query: 'wireless headphones',
                results: 12,
                timestamp: new Date().toISOString()
            }
        ];
    }
    
    getProduct(productId) {
        return this.cache.get(`product:${productId}`);
    }
    
    searchProducts(query) {
        const products = Array.from(this.cache.values())
            .filter(product => product.type === 'product')
            .filter(product => 
                product.name.toLowerCase().includes(query.toLowerCase()) ||
                product.description.toLowerCase().includes(query.toLowerCase())
            );
        
        return products.slice(0, 20); // Return top 20 results
    }
}

module.exports = EdgeSyncService;
```

---

## ü§ñ Part 5: AI-Driven Container Management

### Intelligent Container Orchestration

**AI-Powered Features:**
- Predictive scaling based on traffic patterns
- Intelligent resource allocation
- Automated performance optimization
- Self-healing systems

**AI Container Manager:**
```python
# ai-container-manager.py - AI-driven container management
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
import docker
import time
from datetime import datetime, timedelta

class AIContainerManager:
    def __init__(self):
        self.client = docker.from_env()
        self.scaler = StandardScaler()
        self.model = RandomForestRegressor(n_estimators=100, random_state=42)
        self.metrics_history = []
        self.is_trained = False
        
    def collect_metrics(self):
        """Collect current system metrics"""
        containers = self.client.containers.list()
        
        metrics = {
            'timestamp': datetime.now(),
            'total_containers': len(containers),
            'cpu_usage': 0,
            'memory_usage': 0,
            'network_io': 0,
            'request_rate': self.get_request_rate(),
            'response_time': self.get_avg_response_time()
        }
        
        # Aggregate container metrics
        for container in containers:
            try:
                stats = container.stats(stream=False)
                
                # CPU usage
                cpu_delta = stats['cpu_stats']['cpu_usage']['total_usage'] - \
                           stats['precpu_stats']['cpu_usage']['total_usage']
                system_delta = stats['cpu_stats']['system_cpu_usage'] - \
                              stats['precpu_stats']['system_cpu_usage']
                
                if system_delta > 0:
                    cpu_percent = (cpu_delta / system_delta) * 100.0
                    metrics['cpu_usage'] += cpu_percent
                
                # Memory usage
                memory_usage = stats['memory_stats'].get('usage', 0)
                metrics['memory_usage'] += memory_usage
                
            except Exception as e:
                print(f"Error collecting stats for {container.name}: {e}")
        
        self.metrics_history.append(metrics)
        
        # Keep only last 1000 entries
        if len(self.metrics_history) > 1000:
            self.metrics_history = self.metrics_history[-1000:]
        
        return metrics
    
    def train_model(self):
        """Train AI model on historical data"""
        if len(self.metrics_history) < 50:
            print("Not enough data to train model")
            return
        
        # Prepare training data
        df = pd.DataFrame(self.metrics_history)
        
        # Feature engineering
        df['hour'] = df['timestamp'].dt.hour
        df['day_of_week'] = df['timestamp'].dt.dayofweek
        df['is_weekend'] = df['day_of_week'].isin([5, 6])
        
        # Features for prediction
        features = ['hour', 'day_of_week', 'is_weekend', 'request_rate', 
                   'response_time', 'cpu_usage', 'memory_usage']
        
        X = df[features].fillna(0)
        y = df['total_containers']
        
        # Scale features
        X_scaled = self.scaler.fit_transform(X)
        
        # Train model
        self.model.fit(X_scaled, y)
        self.is_trained = True
        
        print("‚úÖ AI model trained successfully")
    
    def predict_required_containers(self, future_minutes=30):
        """Predict required container count"""
        if not self.is_trained:
            return None
        
        # Create future timestamp
        future_time = datetime.now() + timedelta(minutes=future_minutes)
        
        # Prepare features for prediction
        features = {
            'hour': future_time.hour,
            'day_of_week': future_time.weekday(),
            'is_weekend': future_time.weekday() in [5, 6],
            'request_rate': self.get_request_rate(),
            'response_time': self.get_avg_response_time(),
            'cpu_usage': self.metrics_history[-1]['cpu_usage'] if self.metrics_history else 0,
            'memory_usage': self.metrics_history[-1]['memory_usage'] if self.metrics_history else 0
        }
        
        # Scale features
        X = np.array([[features[f] for f in ['hour', 'day_of_week', 'is_weekend', 
                                           'request_rate', 'response_time', 
                                           'cpu_usage', 'memory_usage']]])
        X_scaled = self.scaler.transform(X)
        
        # Make prediction
        prediction = self.model.predict(X_scaled)[0]
        
        return max(1, int(prediction))
    
    def auto_scale_containers(self, service_name):
        """Automatically scale containers based on AI prediction"""
        predicted_count = self.predict_required_containers()
        
        if predicted_count is None:
            print("Cannot predict - model not trained")
            return
        
        current_containers = len([
            c for c in self.client.containers.list() 
            if service_name in c.name
        ])
        
        print(f"ü§ñ AI Prediction: {predicted_count} containers needed")
        print(f"üìä Current: {current_containers} containers")
        
        if predicted_count > current_containers:
            # Scale up
            scale_up_count = predicted_count - current_containers
            print(f"üìà Scaling up by {scale_up_count} containers")
            self.scale_up_service(service_name, scale_up_count)
            
        elif predicted_count < current_containers:
            # Scale down
            scale_down_count = current_containers - predicted_count
            print(f"üìâ Scaling down by {scale_down_count} containers")
            self.scale_down_service(service_name, scale_down_count)
        
        else:
            print("‚úÖ Current scale is optimal")
    
    def scale_up_service(self, service_name, count):
        """Scale up service containers"""
        for i in range(count):
            container_name = f"{service_name}-{int(time.time())}-{i}"
            
            self.client.containers.run(
                f"ecommerce/{service_name}:latest",
                name=container_name,
                detach=True,
                labels={'service': service_name, 'auto-scaled': 'true'}
            )
            
            print(f"‚úÖ Started container: {container_name}")
    
    def scale_down_service(self, service_name, count):
        """Scale down service containers"""
        containers = [
            c for c in self.client.containers.list() 
            if service_name in c.name and c.labels.get('auto-scaled') == 'true'
        ]
        
        containers_to_remove = containers[:count]
        
        for container in containers_to_remove:
            container.stop()
            container.remove()
            print(f"üóëÔ∏è Removed container: {container.name}")
    
    def get_request_rate(self):
        """Get current request rate (mock implementation)"""
        # In real implementation, this would query monitoring system
        return np.random.normal(100, 20)  # Mock data
    
    def get_avg_response_time(self):
        """Get average response time (mock implementation)"""
        # In real implementation, this would query monitoring system
        return np.random.normal(200, 50)  # Mock data
    
    def run_ai_management_loop(self):
        """Main AI management loop"""
        print("ü§ñ Starting AI container management...")
        
        while True:
            try:
                # Collect metrics
                metrics = self.collect_metrics()
                print(f"üìä Collected metrics: {metrics['total_containers']} containers")
                
                # Train model periodically
                if len(self.metrics_history) % 50 == 0:
                    self.train_model()
                
                # Auto-scale if model is trained
                if self.is_trained:
                    self.auto_scale_containers('ecommerce-api')
                
                time.sleep(60)  # Check every minute
                
            except KeyboardInterrupt:
                print("Stopping AI management...")
                break
            except Exception as e:
                print(f"Error in AI management loop: {e}")
                time.sleep(60)

if __name__ == "__main__":
    manager = AIContainerManager()
    manager.run_ai_management_loop()
```

---

## üéì Module Summary

You've explored the future of containers by learning about:

**Emerging Technologies:**
- WebAssembly integration for enhanced performance
- Next-generation serverless container platforms
- Edge computing with container orchestration
- AI-driven container management and optimization

**Key Trends:**
- Reduced resource overhead and faster cold starts
- Enhanced security through better isolation
- Intelligent automation and self-healing systems
- Seamless multi-cloud and edge deployments

**Preparation Strategies:**
- Stay updated with emerging container technologies
- Experiment with WebAssembly and serverless platforms
- Implement AI-driven optimization where applicable
- Design for edge computing scenarios

**Next Steps:**
- Experiment with emerging technologies in development
- Plan migration strategies for new platforms
- Prepare for Module 16: Advanced Hands-On Labs

---

## üìö Additional Resources

- [WebAssembly System Interface (WASI)](https://wasi.dev/)
- [Knative Serverless Platform](https://knative.dev/)
- [Edge Computing with Kubernetes](https://kubernetes.io/docs/concepts/cluster-administration/networking/)
- [Container Technology Roadmaps](https://www.cncf.io/projects/)
    
    pub fn load_module(&mut self, name: &str, wasm_bytes: &[u8]) -> Result<()> {
        let module = Module::new(&self.engine, wasm_bytes)?;
        self.modules.insert(name.to_string(), module);
        Ok(())
    }
    
    pub fn create_instance(&mut self, name: &str, module_name: &str) -> Result<()> {
        let module = self.modules.get(module_name)
            .ok_or_else(|| anyhow::anyhow!("Module not found"))?;
            
        let store = Store::new(&self.engine, ());
        let instance = Instance::new(&mut store, module, &[])?;
        
        self.instances.insert(name.to_string(), instance);
        Ok(())
    }
    
    pub fn execute_function(&self, instance_name: &str, func_name: &str, args: &[Val]) -> Result<Vec<Val>> {
        let instance = self.instances.get(instance_name)
            .ok_or_else(|| anyhow::anyhow!("Instance not found"))?;
            
        let func = instance.get_func(&mut store, func_name)
            .ok_or_else(|| anyhow::anyhow!("Function not found"))?;
            
        let mut results = vec![Val::I32(0); func.ty(&store).results().len()];
        func.call(&mut store, args, &mut results)?;
        
        Ok(results)
    }
}

// WASI integration for system calls
use wasmtime_wasi::{WasiCtx, WasiCtxBuilder};

pub struct WasiContainer {
    store: Store<WasiCtx>,
    instance: Instance,
}

impl WasiContainer {
    pub fn new(wasm_bytes: &[u8]) -> Result<Self> {
        let engine = Engine::default();
        let module = Module::new(&engine, wasm_bytes)?;
        
        let wasi = WasiCtxBuilder::new()
            .inherit_stdio()
            .inherit_args()?
            .build();
            
        let mut store = Store::new(&engine, wasi);
        
        let mut linker = Linker::new(&engine);
        wasmtime_wasi::add_to_linker(&mut linker, |s| s)?;
        
        let instance = linker.instantiate(&mut store, &module)?;
        
        Ok(WasiContainer { store, instance })
    }
    
    pub fn run_main(&mut self) -> Result<()> {
        let start = self.instance.get_typed_func::<(), ()>(&mut self.store, "_start")?;
        start.call(&mut self.store, ())?;
        Ok(())
    }
}
```

## ‚òÅÔ∏è Serverless Containers

### Serverless Container Platform
```python
#!/usr/bin/env python3
# serverless-containers.py - Next-gen serverless container platform

import asyncio
import time
import json
from dataclasses import dataclass
from typing import Dict, List, Optional, Callable
import docker
import aiohttp

@dataclass
class ServerlessFunction:
    name: str
    image: str
    memory_limit: int
    timeout: int
    environment: Dict[str, str]
    triggers: List[str]

class ServerlessContainerPlatform:
    def __init__(self):
        self.client = docker.from_env()
        self.functions: Dict[str, ServerlessFunction] = {}
        self.cold_start_cache: Dict[str, dict] = {}
        self.execution_stats: Dict[str, list] = {}
        
    async def deploy_function(self, func: ServerlessFunction) -> bool:
        """Deploy serverless function"""
        
        print(f"Deploying serverless function: {func.name}")
        
        try:
            # Pre-warm container for faster cold starts
            await self.pre_warm_container(func)
            
            # Register function
            self.functions[func.name] = func
            
            # Setup triggers
            await self.setup_triggers(func)
            
            print(f"Function {func.name} deployed successfully")
            return True
            
        except Exception as e:
            print(f"Failed to deploy function {func.name}: {e}")
            return False
            
    async def pre_warm_container(self, func: ServerlessFunction):
        """Pre-warm container to reduce cold start time"""
        
        container = self.client.containers.run(
            func.image,
            command="sleep infinity",
            environment=func.environment,
            mem_limit=f"{func.memory_limit}m",
            detach=True,
            remove=False,
            labels={'function': func.name, 'type': 'prewarmed'}
        )
        
        # Pause container to save resources
        container.pause()
        
        self.cold_start_cache[func.name] = {
            'container': container,
            'created_at': time.time()
        }
        
    async def invoke_function(self, func_name: str, event: dict, 
                            context: dict = None) -> dict:
        """Invoke serverless function"""
        
        start_time = time.time()
        
        if func_name not in self.functions:
            return {'error': 'Function not found', 'statusCode': 404}
            
        func = self.functions[func_name]
        
        try:
            # Get or create container
            container = await self.get_execution_container(func)
            
            # Execute function
            result = await self.execute_function(container, func, event, context)
            
            # Record execution stats
            execution_time = time.time() - start_time
            self.record_execution_stats(func_name, execution_time, result)
            
            # Cleanup if needed
            await self.cleanup_container(container, func)
            
            return result
            
        except Exception as e:
            return {
                'error': str(e),
                'statusCode': 500,
                'executionTime': time.time() - start_time
            }
            
    async def get_execution_container(self, func: ServerlessFunction):
        """Get container for function execution"""
        
        # Try to use pre-warmed container
        if func.name in self.cold_start_cache:
            cache_entry = self.cold_start_cache[func.name]
            container = cache_entry['container']
            
            # Check if container is still valid
            if time.time() - cache_entry['created_at'] < 300:  # 5 minutes
                container.unpause()
                return container
            else:
                # Remove expired container
                container.remove(force=True)
                del self.cold_start_cache[func.name]
                
        # Create new container (cold start)
        container = self.client.containers.run(
            func.image,
            environment=func.environment,
            mem_limit=f"{func.memory_limit}m",
            detach=True,
            remove=False,
            labels={'function': func.name, 'type': 'execution'}
        )
        
        return container
        
    async def execute_function(self, container, func: ServerlessFunction, 
                             event: dict, context: dict) -> dict:
        """Execute function in container"""
        
        # Prepare execution environment
        execution_env = {
            'EVENT': json.dumps(event),
            'CONTEXT': json.dumps(context or {}),
            'FUNCTION_NAME': func.name,
            'MEMORY_LIMIT': str(func.memory_limit),
            'TIMEOUT': str(func.timeout)
        }
        
        # Execute function with timeout
        try:
            exec_result = container.exec_run(
                cmd="python /app/handler.py",
                environment=execution_env,
                timeout=func.timeout
            )
            
            if exec_result.exit_code == 0:
                # Parse function output
                output = exec_result.output.decode('utf-8')
                try:
                    result = json.loads(output)
                except json.JSONDecodeError:
                    result = {'body': output, 'statusCode': 200}
            else:
                result = {
                    'error': exec_result.output.decode('utf-8'),
                    'statusCode': 500
                }
                
        except Exception as e:
            result = {'error': f'Execution timeout: {str(e)}', 'statusCode': 408}
            
        return result
        
    async def cleanup_container(self, container, func: ServerlessFunction):
        """Cleanup container after execution"""
        
        # For high-frequency functions, keep container warm
        if self.should_keep_warm(func.name):
            container.pause()
            self.cold_start_cache[func.name] = {
                'container': container,
                'created_at': time.time()
            }
        else:
            container.remove(force=True)
            
    def should_keep_warm(self, func_name: str) -> bool:
        """Determine if container should be kept warm"""
        
        if func_name not in self.execution_stats:
            return False
            
        recent_executions = [
            exec_time for exec_time in self.execution_stats[func_name]
            if time.time() - exec_time < 300  # Last 5 minutes
        ]
        
        # Keep warm if more than 5 executions in last 5 minutes
        return len(recent_executions) > 5
        
    def record_execution_stats(self, func_name: str, execution_time: float, result: dict):
        """Record execution statistics"""
        
        if func_name not in self.execution_stats:
            self.execution_stats[func_name] = []
            
        self.execution_stats[func_name].append({
            'timestamp': time.time(),
            'execution_time': execution_time,
            'status_code': result.get('statusCode', 200),
            'memory_used': result.get('memoryUsed', 0)
        })
        
        # Keep only last 1000 executions
        if len(self.execution_stats[func_name]) > 1000:
            self.execution_stats[func_name] = self.execution_stats[func_name][-1000:]
            
    async def setup_triggers(self, func: ServerlessFunction):
        """Setup function triggers"""
        
        for trigger in func.triggers:
            if trigger.startswith('http'):
                await self.setup_http_trigger(func, trigger)
            elif trigger.startswith('schedule'):
                await self.setup_schedule_trigger(func, trigger)
            elif trigger.startswith('queue'):
                await self.setup_queue_trigger(func, trigger)
                
    async def setup_http_trigger(self, func: ServerlessFunction, trigger: str):
        """Setup HTTP trigger"""
        # Implementation would setup HTTP endpoint
        print(f"HTTP trigger setup for {func.name}: {trigger}")
        
    async def setup_schedule_trigger(self, func: ServerlessFunction, trigger: str):
        """Setup scheduled trigger"""
        # Implementation would setup cron job
        print(f"Schedule trigger setup for {func.name}: {trigger}")
        
    async def setup_queue_trigger(self, func: ServerlessFunction, trigger: str):
        """Setup queue trigger"""
        # Implementation would setup message queue listener
        print(f"Queue trigger setup for {func.name}: {trigger}")

# Edge Computing Integration
class EdgeContainerOrchestrator:
    def __init__(self):
        self.edge_nodes: Dict[str, dict] = {}
        self.workload_placement: Dict[str, str] = {}
        
    async def register_edge_node(self, node_id: str, location: dict, capabilities: dict):
        """Register edge computing node"""
        
        self.edge_nodes[node_id] = {
            'location': location,
            'capabilities': capabilities,
            'workloads': [],
            'last_seen': time.time()
        }
        
    async def deploy_to_edge(self, workload: dict, placement_strategy: str = 'latency'):
        """Deploy workload to optimal edge node"""
        
        if placement_strategy == 'latency':
            target_node = await self.find_lowest_latency_node(workload)
        elif placement_strategy == 'capacity':
            target_node = await self.find_highest_capacity_node(workload)
        else:
            target_node = await self.find_balanced_node(workload)
            
        if target_node:
            await self.deploy_workload_to_node(workload, target_node)
            return target_node
        else:
            raise Exception("No suitable edge node found")
            
    async def find_lowest_latency_node(self, workload: dict) -> Optional[str]:
        """Find edge node with lowest latency to user"""
        
        user_location = workload.get('user_location', {})
        best_node = None
        lowest_latency = float('inf')
        
        for node_id, node_info in self.edge_nodes.items():
            latency = self.calculate_latency(user_location, node_info['location'])
            
            if latency < lowest_latency and self.can_accommodate(node_id, workload):
                lowest_latency = latency
                best_node = node_id
                
        return best_node
        
    def calculate_latency(self, loc1: dict, loc2: dict) -> float:
        """Calculate network latency between locations"""
        # Simplified calculation based on geographic distance
        import math
        
        lat1, lon1 = loc1.get('lat', 0), loc1.get('lon', 0)
        lat2, lon2 = loc2.get('lat', 0), loc2.get('lon', 0)
        
        # Haversine formula for distance
        dlat = math.radians(lat2 - lat1)
        dlon = math.radians(lon2 - lon1)
        a = math.sin(dlat/2)**2 + math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin(dlon/2)**2
        c = 2 * math.asin(math.sqrt(a))
        distance_km = 6371 * c
        
        # Approximate latency: 1ms per 100km + base latency
        return (distance_km / 100) + 5
        
    def can_accommodate(self, node_id: str, workload: dict) -> bool:
        """Check if node can accommodate workload"""
        
        node = self.edge_nodes[node_id]
        required_resources = workload.get('resources', {})
        
        # Check CPU
        if required_resources.get('cpu', 0) > node['capabilities'].get('available_cpu', 0):
            return False
            
        # Check memory
        if required_resources.get('memory', 0) > node['capabilities'].get('available_memory', 0):
            return False
            
        return True

# AI-Driven Container Orchestration
class AIContainerOrchestrator:
    def __init__(self):
        self.ml_model = None
        self.training_data = []
        
    async def predict_optimal_placement(self, workload: dict) -> dict:
        """Use AI to predict optimal container placement"""
        
        if not self.ml_model:
            await self.train_initial_model()
            
        # Extract features from workload
        features = self.extract_workload_features(workload)
        
        # Predict optimal placement
        prediction = self.ml_model.predict([features])
        
        return {
            'recommended_node': prediction[0],
            'confidence': 0.85,
            'reasoning': 'Based on historical performance data'
        }
        
    def extract_workload_features(self, workload: dict) -> list:
        """Extract ML features from workload"""
        
        return [
            workload.get('cpu_requirement', 0),
            workload.get('memory_requirement', 0),
            workload.get('network_intensity', 0),
            workload.get('io_intensity', 0),
            workload.get('priority', 0)
        ]
        
    async def train_initial_model(self):
        """Train initial ML model"""
        # Mock implementation - would use real ML framework
        print("Training AI model for container orchestration...")
        self.ml_model = MockMLModel()

class MockMLModel:
    def predict(self, features):
        return ['node-1']  # Mock prediction

async def main():
    # Serverless containers example
    platform = ServerlessContainerPlatform()
    
    func = ServerlessFunction(
        name="image-processor",
        image="my-image-processor:latest",
        memory_limit=512,
        timeout=30,
        environment={"STAGE": "prod"},
        triggers=["http:/api/process", "schedule:0 */5 * * *"]
    )
    
    await platform.deploy_function(func)
    
    # Invoke function
    result = await platform.invoke_function("image-processor", {"image_url": "https://example.com/image.jpg"})
    print(f"Function result: {result}")

if __name__ == "__main__":
    asyncio.run(main())
```
